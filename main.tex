\documentclass[a4paper]{article}
\input{head}
\begin{document}

%-------------------------------
%	TITLE SECTION
%-------------------------------

\fancyhead[C]{}
\hrule \medskip % Upper rule
\begin{minipage}{0.295\textwidth} 
\raggedright
\footnotesize
YUCHEN WU\hfill\\   
1002060244\hfill\\
cheney.wu@mail.utoronto.ca
\end{minipage}
\begin{minipage}{0.4\textwidth} 
\centering 
\large 
Assignment 2\\ 
\normalsize 
AER1513 Fall, 2020\\ 
\end{minipage}
\begin{minipage}{0.295\textwidth} 
\raggedleft
\today\hfill\\
\end{minipage}
\medskip\hrule 
\bigskip

%-------------------------------
%	CONTENTS
%-------------------------------

\newcommand{\vect}[1]{\bm{\mathbf{#1}}}
\newcommand{\mat}[1]{\bm{\mathbf{#1}}}

\section*{Question 1} 
\textbf{TODO}\\
% As seen from Figure 2.5 in the assignment, all four sources of errors, i.e. range, bearing, translational speed and rotational speed errors, are clearly unimodal and have close-to-zero mean with a spread. Range error and translational speed error align well with the corresponding fitted Gaussian distributions. It looks like bearing error and rotational speed error have a lighter tail than the fitted Gaussian. Overall, the assumption of zero-mean Gaussian noise sounds reasonable. The standard deviation of each source of errors are given in the figure.
% \begin{eqnarray*}
%     \mat{Q}_k & = \begin{bmatrix} q^v_k & 0 \\ 0 & q^\omega_k \end{bmatrix} 
%                 = \begin{bmatrix}
%                 (0.066485)^2  & 0\\
%                 0 & (0.090477)^2 
%               \end{bmatrix} \approx
%               \begin{bmatrix}
%                 0.004420  & 0\\
%                 0 & 0.008186
%               \end{bmatrix}
%               \\
%     \mat{R}_k^l & = \begin{bmatrix} r^r_k & 0 \\ 0 & r^b_k \end{bmatrix} 
%                 = \begin{bmatrix}
%                 (0.030006)^2  & 0\\
%                 0 & (0.025912)^2
%               \end{bmatrix} \approx
%               \begin{bmatrix}
%                 0.000900 & 0\\
%                 0 & 0.000671 
%               \end{bmatrix}
% \end{eqnarray*}
% where $k = 0, ..., K$ and $l = 1, ..., L$.

% Note that in contrast to Assignment 1, we choose not to scale the noise $\mat{Q}_k$ by $T^2=0.01$ because we consider $T$ as part of the nonlinear motion model, and the noise $\vect{w}_k$ is affected by the model. We will discuss how it is affected later.

\section*{Question 2}

We combine the translation vector $\vect{r}_i^{v_k i}$ and rotation matrix $\mat{C}_{v_k i}$ into a pose matrix
\begin{equation}
\mat{T}_k = \mat{T}_{v_k i} = \begin{bmatrix}
  \mat{C}_{v_k i} & -\mat{C}_{v_k i} \vect{r}_i^{v_k i} \\ \vect{0}^T & 1
\end{bmatrix}.
\end{equation}
The state to be estimated is
\begin{equation}
    \vect{x}_{k_1: k_2} = \left\{ \vect{r}_i^{v_{k_1} i}, \mat{C}_{v_{k_1} i}, ..., \vect{r}_i^{v_{k_2} i}, \mat{C}_{v_{k_2} i} \right\} = \left\{ \mat{T}_{v_{k_1} i}, ..., \mat{T}_{v_{k_2} i} \right\}
\end{equation}
Similarly, we combine the translational velocity, $\vect{\nu}_{v_k}^{i v_k}$, and angular velocity of the vehicle, $\vect{\omega}_{v_k}^{i v_k}$, as 
\begin{equation}
    \vect{\varpi} = \begin{bmatrix}
      \vect{\nu}_{v_k}^{i v_k} \\ \vect{\omega}_{v_k}^{i v_k}
    \end{bmatrix}.
\end{equation}
The inputs from time step $k_1$ to $k_2$ can be written using the shorthand
\begin{equation}
    \vect{v} = \{ \vect{\varpi}_{k_1+1}, ..., \vect{\varpi}_{k_1+1} \}.
\end{equation}
Assume that, at time step $k$, $M_k$ landmarks are observed. The measurements can be written as
\begin{equation}
    \vect{y} = \left\{ \vect{y}^{1}_{k_1}, ..., \vect{y}^{M_{k_1}}_{k_1}, ..., \vect{y}^{1}_{k_2}, ..., \vect{y}^{M_{k_2}}_{k_2} \right\} 
\end{equation}
where $\vect{y}_k^j$ is the pixel coordinates of the point $p_j$, projected into the left and right images of the stereo camera $(u_l, v_l)$ and $(u_r, v_r)$ at time $k$, respectively.

Now we define the error terms of the inputs and measurements. For the input $\vect{\varpi}_k$, we have 
\begin{equation}
    \vect{e}_{v, k}(\vect{x}) = \ln (\underbrace{\exp(\Delta t_k \vect{\varpi}_k^\wedge)}_{\mat{\Xi}_k} \mat{T}_{k-1} \mat{T}_k^{-1})^\vee.
\end{equation}

For the measurement, $\vect{y}^j_k$, we have
\begin{equation}
    \vect{e}_{y, jk}(\vect{x}) = \vect{y}^j_k - \bar{\vect{g}}(\vect{p}_{c_k}^{p_j c_k}) = \vect{y}^j_k - \bar{\vect{g}}(\mat{D} \mat{T}_{cv} \mat{T}_{k}\vect{p}_i^{p_j, i})
\end{equation}
where $\bar{\vect{g}}$ is the nominal observation model that projects $\vect{p}_{c_k}^{p_j c_k}$ into the rectified images of an axis-aligned stereo camera, and 
\begin{eqnarray}
    \mat{D} = \begin{bmatrix}
      1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 
    \end{bmatrix}, 
    \mat{T}_{cv} = \begin{bmatrix}
      \mat{C}_{cv} & -\mat{C}_{cv}\vect{\rho}_v^{cv} \\ \vect{0}^T & 1
    \end{bmatrix},
    \vect{p}_i^{p_j, i} = \begin{bmatrix}
      \rho_i^{p_j, i} \\ 1
    \end{bmatrix}
\end{eqnarray}
The weight of input and measurement errors are $\mat{Q}_k^{-1}$ and ${\mat{R}^j_k}^{-1}$, respectively, defined in Questions 1.

Finally, we define the least-squares objective function that we seek to minimize as
\begin{equation}
    J(\vect{x}_{k_1: k_2}) := \dfrac{1}{2} \vect{e}(\vect{x}_{k_1: k_2})^T \mat{T}^{-1} \vect{e}(\vect{x}_{k_1: k_2}),
\end{equation}
where we stack all the error terms and weighting matrices,
\begin{eqnarray*}
    \vect{e}(\vect{x}_{k_1:k_2}) &=& \begin{bmatrix}
      \underbrace{\vect{e}_{v, k_1+1}(\vect{x}_{k_1:k_2})\; ...\;\vect{e}_{v, k_2}(\vect{x}_{k_1:k_2})}_{\text{input errors}} \;\;
      \overbrace{\underbrace{\vect{e}_{y,1 k_1}(\vect{x}_{k_1})\;...\;\vect{e}_{y,M_{k_1} k_1}(\vect{x}_{k_1})}_{\text{measurement errors at } k_1} \;\; ... \;\; \underbrace{\vect{e}_{y,1 k_1}(\vect{x}_{k_2})\;...\;\vect{e}_{y,M_{k_2} k_2}(\vect{x}_{k_1})}_{\text{measurement errors at } k_2}}^{\text{measurement errors}}
    \end{bmatrix}\\
    \mat{T}^{-1} &=& \text{diag}(
      \mat{Q}_{k_1+1}^{-1}\; ...\; \mat{Q}_{k_2}^{-1} \;\; {\mat{R}_{k_1}^{1}}^{-1}\;...\;{\mat{R}_{k_1}^{M_{k_1}}}^{-1}\;\;...\;\;{\mat{R}_{k_2}^{1}}^{-1}\;...\;{\mat{R}_{k_2}^{M_{k_2}}}^{-1}
    )
\end{eqnarray*}


\section*{Question 3}

We first linearize the input and measurement errors at the operating point $\vect{x}_{\text{op}}$. 
Consider 
\begin{equation}
    \mat{T}_k = \exp\left( \vect{\epsilon}_k^\wedge \right).
\end{equation}
For input errors, the linearization is given by
\begin{equation}
    \vect{e}_{v, k}(\vect{x}) \approx \vect{e}_{v, k}(\vect{x}_{\text{op}}) + \underbrace{\text{Ad}\left( \mat{T}_{\text{op}, k} \mat{T}^{-1}_{\text{op}, k-1} \right)}_{\mat{F}_{k-1}} \vect{\epsilon}_{k-1} - \vect{\epsilon}_{k}
\end{equation}
where $\vect{e}_{v, k}(\vect{x}_{\text{op}}) = \ln (\mat{\Xi}_k \mat{T}_{\text{op},k-1} \mat{T}_{\text{op},k}^{-1})^\vee$ is the error evaluated at the operating point.

For measurement errors, we have that
\begin{eqnarray}
    \vect{e}_{y, jk}(\vect{x}) 
    &=& \vect{y}^j_k - \bar{\vect{g}}(\vect{p}_{c_k}^{p_j c_k})\\
    &=& \vect{y}^j_k - \bar{\vect{g}}(\mat{D} \mat{T}_{cv} \mat{T}_{k}\vect{p}_i^{p_j, i})\\
    &\approx& \vect{y}^j_k - \bar{\vect{g}}\left(\mat{D} \mat{T}_{cv}  \exp(\vect{\epsilon}_k^\wedge) \mat{T}_{\text{OP}, k}\vect{p}_i^{p_j, i}  \right)\\
    &\approx& \vect{y}^j_k - \bar{\vect{g}}\left(  \mat{D} \mat{T}_{cv}  (\vect{1} + \vect{\epsilon}_k^\wedge) \mat{T}_{\text{OP}, k}\vect{p}_i^{p_j, i}  \right)\\    
    &\approx& \vect{y}^j_k - \bar{\vect{g}}\left(  \mat{D} \mat{T}_{cv} \mat{T}_{\text{OP}, k}\vect{p}_i^{p_j, i} + \left( \mat{D} \mat{T}_{cv} (\mat{T}_{\text{OP}, k}\vect{p}_i^{p_j, i})^{\odot}  \right) \vect{\epsilon}_k \right)\\    
    &\approx& \underbrace{\vect{y}^j_k - \bar{\vect{g}}\left(  \mat{D} \mat{T}_{cv} \mat{T}_{\text{OP}, k}\vect{p}_i^{p_j, i}\right)}_{\vect{e}_{y, jk}(\vect{x}_{\text{op}})} \underbrace{- \dfrac{\partial \bar{\vect{g}}}{\partial \vect{z}}\Big|_{\vect{z}=\left(\mat{D} \mat{T}_{cv} \mat{T}_{\text{OP}, k}\vect{p}_i^{p_j, i}\right)} \left( \mat{D} \mat{T}_{cv} (\mat{T}_{\text{OP}, k}\vect{p}_i^{p_j, i})^{\odot}  \right)}_{\mat{G}_{jk}} \vect{\epsilon}_k 
\end{eqnarray}
where
\begin{eqnarray}
    \dfrac{\partial \bar{\vect{g}}}{\partial \vect{z}} = \begin{bmatrix}
      \frac{f_u}{z} & 0 & - \frac{f_u x}{z^2}\\
      0 & \frac{f_v}{z} & - \frac{f_v y}{z^2}\\
      \frac{f_u}{z} & 0 & - \frac{f_u (x-b)}{z^2}\\
      0 & \frac{f_v}{z} & - \frac{f_v y}{z^2}\\
    \end{bmatrix}
    \text{with} \quad \vect{z} = \begin{bmatrix}
      x & y & z
    \end{bmatrix}
\end{eqnarray}

Then, we define the following stacked quantities for the Gauss-Newton setup,
\begin{eqnarray}
    \delta \vect{x} &=& \begin{bmatrix}
      \vect{\epsilon}_{k_1} & \vect{\epsilon}_{k_1+1} & \dots & \vect{\epsilon}_{k_2} 
    \end{bmatrix}^T, \\
    \vect{e}(\vect{x}_{\text{op}}) &=& \begin{bmatrix}
      \vect{e}_{v,k_1}(\vect{x}_{\text{op}})\; ...\;\vect{e}_{v, k_2}(\vect{x}_{\text{op}}) \;\;
      \vect{e}_{y,1 k_1}(\vect{x}_{\text{op}})\;...\;\vect{e}_{y,M_{k_1} k_1}(\vect{x}_{\text{op}}) \;\; ... \;\; \vect{e}_{y,1 k_1}(\vect{x}_{\text{op}})\;...\;\vect{e}_{y,M_{k_2} k_2}(\vect{x}_{\text{op}})
    \end{bmatrix}^T\\
    \mat{H} &=& \begin{bmatrix}
      -\mat{F}_{k_1} & \mat{1}          & &&\\
                     & -\mat{F}_{k_1+1} & \mat{1} \\
      &&& \mat{1} & \\
      &&& \mat{F}_{k_2-1} & \mat{1}\\
      %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
      \mat{G}_{1,k_1}\\
      \mat{G}_{2,k_1}\\
      \vdots\\
      \mat{G}_{M_{k_1},k_1}\\
      &\mat{G}_{1,k_1+1}\\
      &\mat{G}_{2,k_1+1}\\
      &\vdots\\
      &\mat{G}_{M_{k_1+1},k_1+1}\\
      &&\ddots\\
      &&\ddots\\
      &&\ddots\\
      &&\ddots\\
      &&&\mat{G}_{1,k_2}\\
      &&&\mat{G}_{2,k_2}\\
      &&&\vdots\\
      &&&\mat{G}_{M_{k_2},k_2}
    \end{bmatrix},\\
    \mat{T} &=& \text{diag} \left(
      \mat{Q}_{k_1+1}\; ...\; \mat{Q}_{k_2} \;\; {\mat{R}_{k_1}^{1}}\;...\;{\mat{R}_{k_1}^{M_{k_1}}}\;\;...\;\;{\mat{R}_{k_2}^{1}}\;...\;{\mat{R}_{k_2}^{M_{k_2}}} \right)
\end{eqnarray}

The quadratic approximation to the objective function is then
\begin{equation}
    J(\vect{x}) \approx J(\vect{x}_{\text{op}}) - \vect{b}^T \delta \vect{x} + \dfrac{1}{2} \delta \vect{x}^T \mat{A} \delta \vect{x}
\end{equation}
where
\begin{equation}
    \mat{A} = \mat{H}^T \mat{W}^{-1}\mat{H}, \quad \vect{b} = \mat{H}^T \mat{W}^{-1} \vect{e}(\vect{x}_{\text{op}})
\end{equation}

Minimizing with respect to $\delta \vect{x}$, we have 
\begin{equation}
    \mat{A} \delta \vect{x}^* = \vect{b}
\end{equation}
for the optimal perturbation
\begin{equation}
    \delta \vect{x}^* = \begin{bmatrix}
      \vect{\epsilon}_{k_1}^* \\ \vect{\epsilon}_{k_1+1}^* \\ \ddots \\ \vect{\epsilon}_{k_2}^*
    \end{bmatrix}
\end{equation}

Finally, we update our operating point through the original perturbation scheme,
\begin{equation}
    \mat{T}_{\text{op}, k} \leftarrow \exp\left( {\vect{\epsilon}_k^*}^{\wedge} \right) \mat{T}_{\text{op}, k}
\end{equation}

\section*{Question 4}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{code/num_visible.png}
    \caption{Number of visible landmarks.}
    \label{fig:4}
\end{figure}

\section*{Question 5}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{code/batch.png}
    \caption{Batch.}
    \label{fig:5a}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{code/sliding_window_50.png}
    \caption{Sliding window 50.}
    \label{fig:5b}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{code/sliding_window_10.png}
    \caption{Sliding window 10.}
    \label{fig:5c}
\end{figure}


\clearpage
\printbibliography

\appendix

\clearpage
\section{Source Code}
\label{sourcecode}

\begin{lstlisting}[language=Python, basicstyle=\small]
# TODO
\end{lstlisting}

\end{document}
